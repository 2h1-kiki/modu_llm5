{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  주택청약 FAQ 시스템 챗봇 구현 - 문서 전처리 + RAG + Gradio ChatInterface\n",
    "\n",
    "### 학습 목표\n",
    "\n",
    "1. 텍스트 문서를 로드하고 구조화된 Q&A 쌍으로 파싱\n",
    "2. LLM을 활용한 키워드 추출 및 요약 생성\n",
    "3. Chroma 벡터 데이터베이스에 문서 임베딩 저장\n",
    "4. MMR 검색 및 메타데이터 필터링 구현\n",
    "5. RAG 체인 구성 및 문서 관련성 평가\n",
    "6. Gradio를 사용한 대화형 챗봇 인터페이스 구현\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 준비\n",
    "\n",
    "**1. 환경 변수 설정**\n",
    "\n",
    "`.env` 파일에 다음 내용을 추가하세요:\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "**2. 필수 패키지 설치**\n",
    "```bash\n",
    "pip install langchain-openai langchain-community langchain-core\n",
    "pip install langchain-chroma chromadb\n",
    "pip install python-dotenv gradio\n",
    "```\n",
    "\n",
    "**3. 데이터 파일**\n",
    "- `data/housing_faq.txt` 파일 필요 (국토교통부 주택청약 FAQ 50개 Q&A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 추가 필기\n",
    "- metadata 기반 필터링\n",
    "\t- 유사도 비교 + 필터링 검색\n",
    "\t- 필터를 걸었을 때 검색된 결과가 비어있는 경우 callback을 넣어줘서 대비한다.\n",
    "\t- 유사도 비교 함수 : as_retriever()\n",
    "- 필터의 역할\n",
    "\t- 필터를 통해서 검색한 결과를 한번 걸러낸 후 Context에 저장하도록 한다.\n",
    "\t- 불필요한 검색을 하지 않으므로 환각을 줄인다.\n",
    "\t- 필터 종류\n",
    "\t\t- text로 sementic(의미 기반)으로 검색 할 때\n",
    "\t\t- 검색 범위 필터링 \n",
    "\t\t\t- <=, <, >, >= \n",
    "\t\t\t- or, and\n",
    "\t\t\t-  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) LLM 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4.1-mini',      # 사용할 모델\n",
    "    temperature=0.1,            # 낮은 값: 일관된 답변 (0.0~2.0)\n",
    "    top_p=0.9,                  # 토큰 샘플링 확률 임계값 (0.0~1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **문서 전처리 파이프라인**\n",
    "\n",
    "* 문서 전처리의 첫 단계는 데이터 정제로, 원본 문서에서 불필요한 요소(HTML 태그, 특수문자, 중복 내용 등)를 제거하고 텍스트를 표준화하는 과정입니다. 이는 검색 품질과 직결되는 중요한 단계입니다.\n",
    "\n",
    "* 문서 청킹(Chunking)은 긴 문서를 의미 있는 작은 단위로 분할하는 과정으로, 문장 단위나 단락 단위로 나누되 문맥의 연속성을 유지하는 것이 핵심입니다. 이는 검색 정확도와 답변 생성의 품질에 직접적인 영향을 미칩니다.\n",
    "\n",
    "* 임베딩(Embedding) 생성은 텍스트를 고차원의 벡터로 변환하는 과정으로, 문서의 의미적 특성을 수치화하여 효율적인 검색을 가능하게 합니다. 이때 사용되는 임베딩 모델의 선택이 검색 성능을 좌우하는 중요한 요소가 됩니다.\n",
    "\n",
    "* 마지막으로 벡터 데이터베이스 색인화 단계에서는 생성된 임베딩을 효율적으로 저장하고 검색할 수 있는 구조로 변환합니다. 이는 대규모 문서 집합에서도 빠른 검색을 가능하게 하는 핵심 요소입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 문서 로드\n",
    "\n",
    "- 국토교통부 주택청약 FAQ에서 일부 내용(청약자격, 청약통장)을 발췌하여 재가공\n",
    "- Q1 ~ Q50까지 모두 50개의 문답이 포함된 텍스트 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 경기도 과천시에서 공급되는 주택의 해당 주택건설지역의 범위는?\n",
      "A 해당 주택건설지역이란 특별시ㆍ광역시ㆍ특별자치시ㆍ특별자치도(관할 구역 안에 지방자치단체인 시ㆍ군이 없는 특별자치도를 말한다) 또는 시ㆍ군의 행정구역을 말합니다. 따라서, 경기도 과천시에서 공급하는 주택의 경우 과천시가 해당 주택건설지역에 해당됩니다. \n",
      "참고로, 서울특별시에서 공급되는 주택의 경우 서울특별시 전역, 인천광역시의 경우 인천광역시 전역이 해당 주택건설지역에 해당됩니다.\n",
      "\n",
      "Q2 해당 주택건설지역에 거주하고 있지 않다면 청약신청이 불가능한지?\n",
      "A 해당 주택건설지역에 거주하고 있지 않더라도 청약가능지역에서 공급되는 주택에 청약신청이 가능하나, 같은 순위에서는 해당 주택건설지역의 거주자가 우선하여 주택을 공급받게 됩니다.\n",
      "* 서울·인천·경기도 / 대전·세종·충남 / 충북 / 광주·전남 / 전북 / 대구·경북 / 부산·울산·경남 / 강원\n",
      "다만, 수도권 대규모 택지개발지구 등에서 주택이 공급되는 경우 일정 비율의 \n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 설정\n",
    "faq_text_file = \"../data/housing_faq.txt\"\n",
    "\n",
    "# 파일 읽기 - 파이썬 내장 함수 사용\n",
    "with open(faq_text_file, 'r') as f:\n",
    "    faq_text = f.read()\n",
    "\n",
    "# 파일 내용 확인\n",
    "print(faq_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[실습] TextLoader를 사용하여, 텍스트 문서를 로드합니다.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 여기에 코드를 작성하세요.\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "\tpath=\"../data\",              # 파일 경로 - 현재 디렉토리\n",
    "\tglob=faq_text_file,     # 파일 확장자 - txt 파일만 로드\n",
    "\tloader_cls=TextLoader,  # TextLoader, CSVLoader, UnstructuredFileLoader 등 지원 \n",
    "\tshow_progress=True,     # 진행 상태바 표시\n",
    ")\n",
    "\n",
    "dir_docs = dir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# # TODO: TextLoader 클래스를 사용하여 FAQ 텍스트 파일을 로드\n",
    "# loader = None\n",
    "# docs = None\n",
    "# len(docs)\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# TextLoader 클래스를 사용하여 FAQ 텍스트 파일을 로드\n",
    "loader = TextLoader(faq_text_file)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 경기도 과천시에서 공급되는 주택의 해당 주택건설지역의 범위는?\n",
      "A 해당 주택건설지역이란 특별시ㆍ광역시ㆍ특별자치시ㆍ특별자치도(관할 구역 안에 지방자치단체인 시ㆍ군이 없는 특별자치도를 말한다) 또는 시ㆍ군의 행정구역을 말합니다. 따라서, 경기도 과천시에서 공급하는 주택의 경우 과천시가 해당 주택건설지역에 해당됩니다. \n",
      "참고로, 서울특별시에서 공급되는 주택의 경우 서울특별시 전역, 인천광역시의 경우 인천광역시 전역이 해당 주택건설지역에 해당됩니다.\n",
      "\n",
      "Q2 해당 주택건설지역에 거주하고 있지 않다면 청약신청이 불가능한지?\n",
      "A 해당 주택건설지역에 거주하고 있지 않더라도 청약가능지역에서 공급되는 주택에 청약신청이 가능하나, 같은 순위에서는 해당 주택건설지역의 거주자가 우선하여 주택을 공급받게 됩니다.\n",
      "* 서울·인천·경기도 / 대전·세종·충남 / 충북 / 광주·전남 / 전북 / 대구·경북 / 부산·울산·경남 / 강원\n",
      "다만, 수도권 대규모 택지개발지구 등에서 주택이 공급되는 경우 일정 비율의 \n"
     ]
    }
   ],
   "source": [
    "# 문서 확인\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../data/housing_faq.txt'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 메타데이터 확인\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 문서 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 각 질문과 답변을 쌍으로 추출하여 정리 (정규표현식 활용)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_qa_pairs(text):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # 텍스트를 라인별로 분리하고 각 라인의 앞뒤 공백 제거\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    current_question = None\n",
    "    current_answer = []\n",
    "    current_number = None\n",
    "    in_answer = False\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if not line:  # 빈 라인 처리\n",
    "            if in_answer and current_answer and i + 1 < len(lines) and lines[i + 1].startswith('Q'):\n",
    "                # 다음 질문이 시작되기 전 빈 줄이면 현재 QA 쌍 저장\n",
    "                qa_pairs.append({\n",
    "                    'number': current_number,\n",
    "                    'question': current_question,\n",
    "                    'answer': ' '.join(current_answer).strip()\n",
    "                })\n",
    "                in_answer = False\n",
    "                current_answer = []\n",
    "            continue\n",
    "            \n",
    "        # 새로운 질문 확인 (Q 다음에 숫자가 오는 패턴)\n",
    "        q_match = re.match(r'Q(\\d+)\\s+(.*)', line)\n",
    "        if q_match:\n",
    "            # 이전 QA 쌍이 있으면 저장\n",
    "            if current_question is not None and current_answer:\n",
    "                qa_pairs.append({\n",
    "                    'number': current_number,\n",
    "                    'question': current_question,\n",
    "                    'answer': ' '.join(current_answer).strip()\n",
    "                })\n",
    "            \n",
    "            # 새로운 질문 시작\n",
    "            current_number = int(q_match.group(1))\n",
    "            current_question = q_match.group(2).strip().rstrip('?') + '?'  # 질문 마크 정규화\n",
    "            current_answer = []\n",
    "            in_answer = False\n",
    "            \n",
    "        # 답변 시작 확인\n",
    "        elif line.startswith('A ') or (current_question and not current_answer and line):\n",
    "            in_answer = True\n",
    "            current_answer.append(line.lstrip('A '))\n",
    "            \n",
    "        # 기존 답변에 내용 추가\n",
    "        elif current_question is not None and (in_answer or not line.startswith('Q')):\n",
    "            if in_answer or (current_answer and not line.startswith('Q')):\n",
    "                current_answer.append(line)\n",
    "    \n",
    "    # 마지막 QA 쌍 처리\n",
    "    if current_question is not None and current_answer:\n",
    "        qa_pairs.append({\n",
    "            'number': current_number,\n",
    "            'question': current_question,\n",
    "            'answer': ' '.join(current_answer).strip()\n",
    "        })\n",
    "    \n",
    "    # 번호 순서대로 정렬\n",
    "    qa_pairs.sort(key=lambda x: x['number'])\n",
    "    \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 QA 쌍 개수: 50\n",
      "추출된 첫번째 QA: \n",
      "{'number': 1, 'question': '경기도 과천시에서 공급되는 주택의 해당 주택건설지역의 범위는?', 'answer': '해당 주택건설지역이란 특별시ㆍ광역시ㆍ특별자치시ㆍ특별자치도(관할 구역 안에 지방자치단체인 시ㆍ군이 없는 특별자치도를 말한다) 또는 시ㆍ군의 행정구역을 말합니다. 따라서, 경기도 과천시에서 공급하는 주택의 경우 과천시가 해당 주택건설지역에 해당됩니다. 참고로, 서울특별시에서 공급되는 주택의 경우 서울특별시 전역, 인천광역시의 경우 인천광역시 전역이 해당 주택건설지역에 해당됩니다.'}\n"
     ]
    }
   ],
   "source": [
    "# QA 쌍 추출\n",
    "qa_pairs = extract_qa_pairs(docs[0].page_content) \n",
    "\n",
    "print(f\"추출된 QA 쌍 개수: {len(qa_pairs)}\")\n",
    "print(f\"추출된 첫번째 QA: \\n{qa_pairs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) LLM으로 추가 정보를 추출`\n",
    "- 텍스트에서 키워드와 핵심 개념을 추출하는 체인\n",
    "- 메타데이터 or 본문(page_content)에 추가하여 검색에 활용    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드: 해당 주택건설지역\n",
      "요약: 경기도 과천시에서 공급되는 주택의 해당 주택건설지역은 과천시 행정구역 전체를 의미한다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# 출력 형식 정의\n",
    "class KeywordOutput(BaseModel):\n",
    "    keyword: str = Field(description=\"텍스트에서 추출한 가장 중요한 키워드(법률용어, 주제 등))\")\n",
    "    summary: str = Field(description=\"텍스트의 간단한 요약\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"당신은 텍스트 분석 전문가입니다. \n",
    "주어진 텍스트에서 중요한 키워드를 추출하고, 텍스트의 간단한 요약을 작성하는 것이 당신의 역할입니다.\n",
    "\n",
    "## 추출 지침:\n",
    "- 텍스트의 맥락을 고려하여 핵심 용어나 전문 용어를 추출합니다\n",
    "- 주요 아이디어나 원리, 개념을 포함합니다\n",
    "- 가장 중요한 키워드를 1개 추출합니다\n",
    "- 요약은 1문장으로 간결하게 작성합니다\n",
    "\n",
    "## 출력 형식:\n",
    "- keyword: 가장 중요한 키워드 \n",
    "- summary: 텍스트의 간단한 요약\"\"\"),\n",
    "    \n",
    "    (\"user\", \"다음 텍스트를 분석해주세요:\\n\\n{input_text}\")\n",
    "])\n",
    "\n",
    "# LCEL 체인 구성\n",
    "llm_with_structure = llm.with_structured_output(KeywordOutput) \n",
    "keyword_extractor = prompt | llm_with_structure\n",
    "\n",
    "# 텍스트 추출 테스트     \n",
    "result = keyword_extractor.invoke(qa_pairs[0]['question']+qa_pairs[0]['answer'])\n",
    "print(\"키워드:\", result.keyword)\n",
    "print(\"요약:\", result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) QA 쌍을 문자열 포맷팅하고 문서 객체로 변환`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...체를 의미한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...간에 포함된다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...정하지 않는다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...준을 적용한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...으로 산정한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...이 불가능하다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='... 따라 판단된다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...만 가능합니다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...출이 가능하다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...가입해야 한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...득을 확인한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='... 가입할 수 있다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...을 받을 수 있다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...를 받을 수 있다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...납이 가능하다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...가 가능합니다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...까지 적용된다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...은 250만원이다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='... 예치해야 한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...약이 가능하다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...경이 가능하다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...활이 가능하다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='... 가입해야 한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='... 전환해야 한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...자를 결정한다.\"), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...원을 의미한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...을 주지 않는다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='... 미치지 않는다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "포맷팅된 문서 개수: 50\n",
      "[1]\n",
      "질문: 경기도 과천시에서 공급되는 주택의 해당 주택건설지역의 범위는?\n",
      "답변: 해당 주택건설지역이란 특별시ㆍ광역시ㆍ특별자치시ㆍ특별자치도(관할 구역 안에 지방자치단체인 시ㆍ군이 없는 특별자치도를 말한다) 또는 시ㆍ군의 행정구역을 말합니다. 따라서, 경기도 과천시에서 공급하는 주택의 경우 과천시가 해당 주택건설지역에 해당됩니다. 참고로, 서울특별시에서 공급되는 주택의 경우 서울특별시 전역, 인천광역시의 경우 인천광역시 전역이 해당 주택건설지역에 해당됩니다.\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'answer': '해당 주택건설지역이란 특별시ㆍ광역시ㆍ특별자치시ㆍ특별자치도(관할 구역 안에 지방자치단체인 시ㆍ군이 없는 특별자치도를 '\n",
      "           '말한다) 또는 시ㆍ군의 행정구역을 말합니다. 따라서, 경기도 과천시에서 공급하는 주택의 경우 과천시가 해당 '\n",
      "           '주택건설지역에 해당됩니다. 참고로, 서울특별시에서 공급되는 주택의 경우 서울특별시 전역, 인천광역시의 경우 인천광역시 '\n",
      "           '전역이 해당 주택건설지역에 해당됩니다.',\n",
      " 'keyword': '해당 주택건설지역',\n",
      " 'question': '경기도 과천시에서 공급되는 주택의 해당 주택건설지역의 범위는?',\n",
      " 'question_id': 1,\n",
      " 'summary': '경기도 과천시에서 공급되는 주택의 해당 주택건설지역은 과천시 행정구역 전체를 의미한다.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...격이 인정된다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def format_qa_pairs(qa_pairs):\n",
    "    \"\"\"\n",
    "    추출된 QA 쌍을 포맷팅하여 문서 객체로 변환\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    for pair in qa_pairs:\n",
    "\n",
    "        # QA 쌍을 포맷팅\n",
    "        formatted_output = (\n",
    "            f\"[{pair['number']}]\\n\"\n",
    "            f\"질문: {pair['question']}\\n\"\n",
    "            f\"답변: {pair['answer']}\\n\"\n",
    "        )\n",
    "\n",
    "        # 키워드와 요약 추출\n",
    "        result = keyword_extractor.invoke(pair['question']+\"\\n\\n\"+pair['answer'])\n",
    "\n",
    "        # 문서 객체 생성\n",
    "        doc = Document(\n",
    "            page_content=formatted_output,\n",
    "            metadata={\n",
    "                'question_id': int(pair['number']),\n",
    "                'question': pair['question'],\n",
    "                'answer': pair['answer'],\n",
    "                'keyword': result.keyword,\n",
    "                'summary': result.summary\n",
    "            }\n",
    "        )\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "# QA 쌍 포맷팅\n",
    "formatted_docs = format_qa_pairs(qa_pairs)\n",
    "print(f\"포맷팅된 문서 개수: {len(formatted_docs)}\")\n",
    "\n",
    "# 문서 확인\n",
    "print(formatted_docs[0].page_content)\n",
    "print(\"-\" * 200)\n",
    "# 문서 메타데이터 확인\n",
    "pprint(formatted_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "포맷팅된 문서를 ../data/housing_faq_formatted.json에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "# 문서 저장\n",
    "output_file = \"../data/housing_faq_formatted.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump([doc.model_dump() for doc in formatted_docs], f, indent=2, ensure_ascii=False)  # 한글이 유니코드로 변환되지 않도록 설정\n",
    "print(f\"포맷팅된 문서를 {output_file}에 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[실습] 문서 객체를 포맷팅하여 구성합니다.***\n",
    "\n",
    "- 요약문을 시맨틱 검색에 활용합니다. 다음 구조로 문서 객체를 생성합니다. \n",
    "    - page_content: 요약\n",
    "    - metadata: 기타 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def format_qa_pairs_with_summary(qa_pairs):\n",
    "    \"\"\"\n",
    "    추출된 QA 쌍을 포맷팅하여 문서 객체로 변환\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    for pair in qa_pairs:\n",
    "\n",
    "        # 키워드와 요약 추출\n",
    "        result = None\n",
    "\n",
    "        # 문서 객체 생성\n",
    "        doc = None\n",
    "\n",
    "        processed_docs.append(doc)\n",
    "        \n",
    "    return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [강사]\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def format_qa_pairs_with_summary(qa_pairs):\n",
    "    \"\"\"\n",
    "    추출된 QA 쌍을 포맷팅하여 문서 객체로 변환\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    for pair in qa_pairs:\n",
    "\n",
    "        # 키워드와 요약 추출\n",
    "        result = keyword_extractor.invoke(pair['question']+\"\\n\\n\"+pair['answer'])\n",
    "\n",
    "        # 문서 객체 생성\n",
    "        doc = Document(\n",
    "            page_content=result.summary,\n",
    "            metadata={\n",
    "                'question_id': int(pair['number']),\n",
    "                'question': pair['question'],\n",
    "                'answer': pair['answer'],\n",
    "                'keyword': result.keyword,\n",
    "            }\n",
    "        )\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...간을 산정한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...하여 판단한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...상이 될 수 있다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...자가 될 수 없다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...으로 확인한다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...부가 결정된다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/hyewonmac/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=KeywordOutput(keyword='...이 가능합니다.'), input_type=KeywordOutput])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# QA 쌍 포맷팅\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m summary_formatted_docs = \u001b[43mformat_qa_pairs_with_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_pairs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m포맷팅된 문서 개수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(summary_formatted_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 문서 확인\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mformat_qa_pairs_with_summary\u001b[39m\u001b[34m(qa_pairs)\u001b[39m\n\u001b[32m      8\u001b[39m processed_docs = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m qa_pairs:\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# 키워드와 요약 추출\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     result = \u001b[43mkeyword_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43manswer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# 문서 객체 생성\u001b[39;00m\n\u001b[32m     15\u001b[39m     doc = Document(\n\u001b[32m     16\u001b[39m         page_content=result.summary,\n\u001b[32m     17\u001b[39m         metadata={\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m         }\n\u001b[32m     23\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3157\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3155\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3156\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3157\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5695\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5688\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5690\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5693\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5694\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5696\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5697\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5698\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5699\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1123\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1114\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1120\u001b[39m     **kwargs: Any,\n\u001b[32m   1121\u001b[39m ) -> LLMResult:\n\u001b[32m   1122\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:933\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    932\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    939\u001b[39m         )\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    941\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1235\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1233\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1235\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1239\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1445\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload:\n\u001b[32m   1443\u001b[39m     payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1444\u001b[39m     raw_response = (\n\u001b[32m-> \u001b[39m\u001b[32m1445\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1446\u001b[39m     )\n\u001b[32m   1447\u001b[39m     response = raw_response.parse()\n\u001b[32m   1448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_responses_api(payload):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:184\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    179\u001b[39m         response_format=response_format,\n\u001b[32m    180\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    181\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/openai/_base_client.py:1297\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1288\u001b[39m     warnings.warn(\n\u001b[32m   1289\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1291\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1292\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1293\u001b[39m     )\n\u001b[32m   1294\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1295\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1296\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/openai/_base_client.py:1005\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1003\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1011\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/modu_llm_5/modu_llm5_project/001_chatbot/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# QA 쌍 포맷팅\n",
    "summary_formatted_docs = format_qa_pairs_with_summary(qa_pairs) \n",
    "print(f\"포맷팅된 문서 개수: {len(summary_formatted_docs)}\")\n",
    "\n",
    "# 문서 확인\n",
    "print(summary_formatted_docs[0].page_content)\n",
    "print(\"-\" * 200)\n",
    "# 문서 메타데이터 확인\n",
    "pprint(summary_formatted_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 저장\n",
    "output_file = \"../data/housing_faq_formatted_with_summary.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump([doc.model_dump() for doc in summary_formatted_docs], f, indent=2, ensure_ascii=False)  # 한글이 유니코드로 변환되지 않도록 설정\n",
    "print(f\"포맷팅된 문서를 {output_file}에 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 벡터 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 로드\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "output_file = \"../data/housing_faq_formatted.json\"\n",
    "\n",
    "with open(output_file, 'r', encoding='utf-8-sig') as f:\n",
    "    formatted_docs = [Document(**doc) for doc in json.load(f)]\n",
    "\n",
    "# 문서 확인\n",
    "print(formatted_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 문서 벡터 저장\n",
    "vector_store = Chroma.from_documents(  \n",
    "    documents=formatted_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"housing_faq_db\",\n",
    "    persist_directory=\"../chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[실습] 요약 문서(summary_formatted_docs)를 벡터 스토어에 저장합니다.*** \n",
    "\n",
    "- OpenAI (text-embedding-3-small) 임베딩 모델 사용\n",
    "- Chroma DB 사용\n",
    "\n",
    "**힌트**:\n",
    "1. `Chroma.from_documents()` 메소드 사용\n",
    "2. `embedding` 파라미터에 OpenAIEmbeddings 인스턴스 전달\n",
    "3. `collection_name`과 `persist_directory` 지정\n",
    "4. `summary_formatted_docs` 변수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 로드\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "output_file = \"../ata/housing_faq_formatted_with_summary.json\"\n",
    "\n",
    "with open(output_file, 'r', encoding='utf-8-sig') as f:\n",
    "    summary_formatted_docs = [Document(**doc) for doc in json.load(f)]\n",
    "\n",
    "# 문서 확인\n",
    "print(summary_formatted_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 문서 벡터 저장\n",
    "# vector_store_summary = None\n",
    "vector_store_summary = Chroma.from_documents(  \n",
    "    documents=formatted_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"housing_faq_db\",\n",
    "    persist_directory=\"../chroma_db\",\n",
    ")\n",
    "\n",
    "print(vector_store_summary._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 벡터 저장소 로드\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"housing_faq_db\",\n",
    "    persist_directory=\"../chroma_db\", \n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "vector_store._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[실습] 앞에서 저장한 요약문서 벡터 스토어를 로드합니다.*** \n",
    "\n",
    "- OpenAI (text-embedding-3-small) 임베딩 모델 사용\n",
    "- Chroma DB 사용\n",
    "\n",
    "**힌트**:\n",
    "1. `Chroma()` 생성자 사용 (from_documents가 아님)\n",
    "2. 저장 시 사용한 `collection_name`, `persist_directory` 동일하게 지정\n",
    "3. `embedding_function` 파라미터에 OpenAIEmbeddings 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요.\n",
    "\n",
    "\n",
    "# 벡터 저장소 로드\n",
    "# vector_store_summary = None\n",
    "vector_store_summary = Chroma(\n",
    "\t\tcollection_name=\"housing_faq_db\",\n",
    "\t\tpersist_directory=\"../chroma_db\", \n",
    "\t\tembedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색기 생성 - 유사도 기반 상위 3개 문서 검색\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# 테스트 질문\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MMR (Maximal Marginal Relevance)\n",
    "\n",
    "MMR은 검색 결과의 **관련성**과 **다양성**을 동시에 고려하는 알고리즘입니다.\n",
    "\n",
    "**주요 파라미터**:\n",
    "- `fetch_k`: 초기 검색 문서 수 (유사도 기준)\n",
    "- `k`: 최종 반환 문서 수\n",
    "- `lambda_mult`: 다양성 가중치\n",
    "  - `0.0`: 최대 다양성 (서로 다른 문서 우선)\n",
    "  - `1.0`: 최대 관련성 (유사도만 고려)\n",
    "  - `0.5`: 균형 (관련성과 다양성을 동등하게)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[실습] MMR 검색기를 정의합니다.***  \n",
    "\n",
    "- 문서 벡터 스토어를 사용\n",
    "- 10개의 문서를 가져와서, 다양성 기반으로 3개를 선택 (다양성은 중간 수준 적용)\n",
    "\n",
    "**힌트**:\n",
    "1. `vector_store.as_retriever()` 메소드 사용\n",
    "2. `search_type=\"mmr\"` 파라미터 지정\n",
    "3. `search_kwargs`에 `fetch_k=10`, `k=3`, `lambda_mult=0.5` 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요.\n",
    "\n",
    "mmr_retriever = None\n",
    "\n",
    "# 테스트 질문\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "\n",
    "results = mmr_retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(result.metadata['question'])\n",
    "    print(result.metadata['answer'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[심화] 메타데이터 기반 필터링**\n",
    "\n",
    "- Chroma 문서: https://docs.trychroma.com/docs/querying-collections/metadata-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 필드 정확히 일치\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": \"해당 주택건설지역\"}},\n",
    ")\n",
    "\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $eq 연산자 사용 - 정확히 일치 :: 유사도가 아닌 완전히 일치해야만 하는 경우\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$eq\": \"해당 주택건설지역\"}}},\n",
    ")\n",
    "\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ne (Not Equal) 연산자 사용 - 정확히 일치하지 않는 문서 검색\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$ne\": \"해당 주택건설지역\"}}},\n",
    ")\n",
    "\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $in 연산자로 여러 값 중 일치하는 문서 검색\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$in\": [\"해당 주택건설지역\", \"청약예금\"]}}},\n",
    ")\n",
    "\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 범위 검색 ($gt, $gte, $lt, $lte) - question_id가 10 이상인 문서 검색\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"question_id\": {\"$gte\": 10}}},\n",
    ")\n",
    "\n",
    "query = \"무주택자 기준은 무엇인가요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $and로 여러 조건 조합 - keyword가 \"주택건설지역\"이고 question_id가 10 미만인 문서 검색\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"$and\": [\n",
    "        {\"keyword\": \"해당 주택건설지역\"}, \n",
    "        {\"question_id\": {\"$lt\": 10}}\n",
    "    ]}},\n",
    ")\n",
    "\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $or로 여러 조건 중 하나 일치하는 문서 검색 - keyword가 \"주택건설지역\"이거나 question_id가 10 이상인 문서 검색\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"$or\": [\n",
    "        {\"keyword\": \"해당 주택건설지역\"}, \n",
    "        {\"question_id\": {\"$gte\": 10}}\n",
    "    ]}},\n",
    ")\n",
    "\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식 패턴 매칭 - page_content 본문에 \"주택건설지역\"이 포함된 문서 검색\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={'where_document': {'$contains': '해당 주택건설지역'}},\n",
    ")\n",
    "\n",
    "query = \"수원시의 '해당 주택건설지역'은 어디에 해당하나요?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[실습] 메타데이터 필터링 조건을 적용하는 실습을 수행합니다..*** \n",
    "\n",
    "- 요약 문서 벡터 스토어 기반 MMR 검색기에 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메타데이터 필터 LLM 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**추가필기**\n",
    "\n",
    "### 메타데이터 필터 LLM 추출이란?\n",
    "- 질문이 항상 바뀔 수 있는데 필터를 그때마다 하드코딩으로 바꿔줄 수 없으니 이것을 LLM으로 추출하는 방법이다.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal\n",
    "\n",
    "## Schema 상세 정의\n",
    "class MetadataFilter(BaseModel):\n",
    "    \"\"\"Chroma DB 메타데이터 필터 조건\"\"\"\n",
    "\n",
    "    # 키워드 필터\n",
    "    keyword: Optional[str] = Field(default=None, description=\"검색할 키워드\")\n",
    "    keyword_operator: Optional[Literal[\"$eq\", \"$ne\"]] = Field(\n",
    "        default=None, description=\"키워드 비교 연산자\"\n",
    "    )\n",
    "\n",
    "    # 질문 ID 범위 (하한)\n",
    "    question_id_min: Optional[int] = Field(default=None, description=\"질문 ID 최소값\")\n",
    "    question_id_min_operator: Optional[Literal[\"$gt\", \"$gte\"]] = Field(\n",
    "        default=None, description=\"최소값 연산자 ($gt: 초과, $gte: 이상)\"\n",
    "    )\n",
    "\n",
    "    # 질문 ID 범위 (상한)\n",
    "    question_id_max: Optional[int] = Field(default=None, description=\"질문 ID 최대값\")\n",
    "    question_id_max_operator: Optional[Literal[\"$lt\", \"$lte\"]] = Field(\n",
    "        default=None, description=\"최대값 연산자 ($lt: 미만, $lte: 이하)\"\n",
    "    )\n",
    "\n",
    "    # 논리 연산자\n",
    "    logical_operator: Optional[Literal[\"$and\", \"$or\"]] = Field(\n",
    "        default=\"$and\", description=\"복합 조건 결합 연산자\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 시스템 프롬프트\n",
    "METADATA_FILTER_SYSTEM_PROMPT = \"\"\"사용자 쿼리에서 Chroma DB 검색 필터 조건을 추출합니다.\n",
    "\n",
    "## 추출 규칙\n",
    "\n",
    "### 키워드 (keyword)\n",
    "- 특정 단어/주제 검색 시 해당 키워드 추출\n",
    "- keyword_operator: 일반적으로 \"$eq\" 사용\n",
    "\n",
    "### 질문 ID 범위\n",
    "- \"N번 이상\": question_id_min=N, question_id_min_operator=\"$gte\"\n",
    "- \"N번 초과\": question_id_min=N, question_id_min_operator=\"$gt\"\n",
    "- \"N번 이하\": question_id_max=N, question_id_max_operator=\"$lte\"\n",
    "- \"N번 미만\": question_id_max=N, question_id_max_operator=\"$lt\"\n",
    "- \"N~M번 사이\": 최소값과 최대값 모두 설정\n",
    "\n",
    "### 논리 연산자\n",
    "- 모든 조건 만족: \"$and\" (기본값)\n",
    "- 하나라도 만족: \"$or\"\n",
    "\n",
    "## 예시\n",
    "\n",
    "1. 키워드만: \"주택건설 관련 문서\"\n",
    "   → keyword=\"주택건설\", keyword_operator=\"$eq\"\n",
    "\n",
    "2. ID 범위: \"10번 이상 20번 이하\"\n",
    "   → question_id_min=10, question_id_min_operator=\"$gte\",\n",
    "     question_id_max=20, question_id_max_operator=\"$lte\"\n",
    "\n",
    "## 기간에 대해서 검색하는 것으로도 가능 - ex) 벤츠 SUV 몇 년식 이후 모델 검색\n",
    "3. 복합 조건: \"청약통장 관련 40~50번 문서\"\n",
    "   → keyword=\"청약통장\", keyword_operator=\"$eq\",\n",
    "     question_id_min=40, question_id_min_operator=\"$gte\",\n",
    "     question_id_max=50, question_id_max_operator=\"$lte\",\n",
    "     logical_operator=\"$and\"\n",
    "\n",
    "해당 정보가 없으면 null 반환.\n",
    "\"\"\"\n",
    "\n",
    "# 메타데이터 추출 체인 구성\n",
    "metadata_extraction_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", METADATA_FILTER_SYSTEM_PROMPT),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    | llm.with_structured_output(MetadataFilter)\n",
    ")\n",
    "\n",
    "# 테스트: 필터 조건 추출\n",
    "query = \"'해당 주택건설지역' 관련 문서를 10번 이하인 문서중에서 검색해주세요\"\n",
    "filter_params = metadata_extraction_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"추출된 필터 파라미터:\")\n",
    "print(filter_params.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chroma_filter(filter_params: MetadataFilter) -> dict:\n",
    "    \"\"\"MetadataFilter를 Chroma DB 필터 딕셔너리로 변환\n",
    "    \n",
    "    Args:\n",
    "        filter_params: MetadataFilter 인스턴스\n",
    "        \n",
    "    Returns:\n",
    "        Chroma DB where 절에 사용할 필터 딕셔너리\n",
    "    \"\"\"\n",
    "    conditions = []\n",
    "\n",
    "    ### 필터 조건 구성\n",
    "\n",
    "    # 키워드 조건\n",
    "    if filter_params.keyword and filter_params.keyword_operator:\n",
    "        conditions.append({\n",
    "            \"keyword\": {filter_params.keyword_operator: filter_params.keyword}\n",
    "        })\n",
    "\n",
    "    # 질문 ID 최소값 조건\n",
    "    if filter_params.question_id_min is not None and filter_params.question_id_min_operator:\n",
    "        conditions.append({\n",
    "            \"question_id\": {filter_params.question_id_min_operator: filter_params.question_id_min}\n",
    "        })\n",
    "\n",
    "    # 질문 ID 최대값 조건\n",
    "    if filter_params.question_id_max is not None and filter_params.question_id_max_operator:\n",
    "        conditions.append({\n",
    "            \"question_id\": {filter_params.question_id_max_operator: filter_params.question_id_max}\n",
    "        })\n",
    "\n",
    "    # 조건 개수에 따른 필터 구성\n",
    "    if len(conditions) == 0:\n",
    "        return {}\n",
    "    elif len(conditions) == 1:\n",
    "        return conditions[0]\n",
    "    else:\n",
    "        logical_op = filter_params.logical_operator or \"$and\"\n",
    "        return {logical_op: conditions}\n",
    "\n",
    "\n",
    "# 테스트: 필터 딕셔너리 생성\n",
    "filter_dict = build_chroma_filter(filter_params)\n",
    "print(\"생성된 Chroma 필터:\")\n",
    "print(filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def metadata_filter_retriever(query: str):\n",
    "    \"\"\"메타데이터 필터를 추출하고 검색 수행\n",
    "    \n",
    "    Args:\n",
    "        query: 사용자 검색 쿼리\n",
    "        \n",
    "    Returns:\n",
    "        검색된 문서 리스트\n",
    "    \"\"\"\n",
    "    # 1. 필터 조건 추출\n",
    "    filter_params = metadata_extraction_chain.invoke({\"query\": query})\n",
    "\n",
    "    # 2. Chroma 필터로 변환\n",
    "    filter_dict = build_chroma_filter(filter_params)\n",
    "    print(f\"추출된 필터: {filter_dict}\")\n",
    "\n",
    "    # 3. 검색 실행\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_kwargs={\"filter\": filter_dict} if filter_dict else {}\n",
    "    )\n",
    "    return retriever.invoke(query)\n",
    "\n",
    "\n",
    "# 테스트 실행\n",
    "query = \"청약통장 관련 문서를 40번과 50번 사이의 문서 중에서 검색해주세요\"\n",
    "results = metadata_filter_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\n검색된 문서 수: {len(results)}\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- 문서 {i} ---\")\n",
    "    print(f\"내용: {result.page_content[:100]}...\")\n",
    "    print(f\"키워드: {result.metadata.get('keyword', 'N/A')}\")\n",
    "    print(f\"질문 ID: {result.metadata.get('question_id', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 쿼리 테스트 - 모순되지 않은 형태의 메세지로만 구성\n",
    "test_queries = [\n",
    "    \"청약통장 관련 문서 찾아줘\",        # 키워드만\n",
    "    \"질문 ID 10번 이상인 문서\",         # ID 하한만\n",
    "    \"20번 이하 문서만 보여줘\",          # ID 상한만\n",
    "    \"10번에서 30번 사이 문서\",          # ID 범위\n",
    "    \"'해당 주택건설지역' 관련 10번 이하 문서\",  # 복합\n",
    "    \"청약통장 관련 40~50번 문서\",       # 복합 + 범위\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"쿼리: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # 필터 추출 및 검색\n",
    "    filter_params = metadata_extraction_chain.invoke({\"query\": query})\n",
    "    filter_dict = build_chroma_filter(filter_params)\n",
    "    print(f\"필터: {filter_dict}\")\n",
    "    \n",
    "    # 검색 실행\n",
    "    results = metadata_filter_retriever.invoke(query)\n",
    "    print(f\"검색 결과: {len(results)}건\")\n",
    "    \n",
    "    if results:\n",
    "        print(f\"첫 번째 문서 - 키워드: {results[0].metadata.get('keyword')}, ID: {results[0].metadata.get('question_id')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[실습] 메타데이터 필터링 체인의 성능을 개선합니다.*** \n",
    "\n",
    "- (예시)\n",
    "    - 추가 예시 제공을 통해 필터링 적용 법위 확대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**추가필기**\n",
    "\n",
    "### RAG로 실습진행 할 때 주의사항\n",
    "- K값은 적게 하면 안된다. 왜냐?\n",
    "\t- 상대적 정밀도로 10, 20, 30, 50, 100, 200 등등으로 테스트해봐야한다.\n",
    "\t- 예를 들어 데이터가 100만개라고 했을 때 그 중 10개 검색되는 것과 200개 검색되는 것은 다르므로\n",
    "- 정확한 검색을 위해\n",
    "\t- Vector store를 1, 2, 3으로 여러개 만들어서 검색\n",
    "\t\t-> keyword검색 후 hybrid처리 + graph기반 검색 가능\n",
    "- 참조 없이 직접 답변 생성 시\n",
    "\t- 실제 검색을 한건지 확인을 위한 코드\n",
    "\t\t```python\n",
    "\t\t# 결과 출력\n",
    "\t\t\tprint(\"답변:\", result[\"response\"][\"answer\"])\n",
    "\t\t\tprint(\"\\n참조 문서:\")\n",
    "\t\t\tfor i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "\t\t\t\t\tprint(f\"\\n문서 {i}:\")\n",
    "\t\t\t\t\tprint(f\"내용: {doc.page_content}\")\n",
    "\t\t```\n",
    "\t- 해당 검색 결과에서도 관련성 있는 답변만 거르기 위해서 \n",
    "\t\t- 검색 문서 관련성 평가 해당 섹션 확인\n",
    "\t\t- 하지만 이런걸 llm으로 판단 하는 것도 정확하지 않다.\n",
    "\t\t\t- 이유\n",
    "\t\t\t\t1. 여러번 실행 시 답변이 달라진다.\n",
    "\t\t\t- 해결하기 위해서는\n",
    "\t\t\t\t1. 모델을 다른 것으로 사용해본다. > cloud, gemini - 모델이 갖는 지능의 차이가 있다.\n",
    "\t\t\t\t2. 여러 모델을 사용하여 하나로 만들거나\n",
    "- FAQ의 경우에는\n",
    "\t- 임베딩한 문서와 원본을 다르게 전달하는 방식\n",
    "\t- 임베딩을 어떻게 chunk를 어떻게 구성하는게 중요하다.\n",
    "\t- 검색을 할 때 Q, A에서 Q로만 검색되도록 처리하는 방식으로 하면 검색의 정확도가 올라간다.\n",
    "\t- 원본에 대한 요약 (요약을 목차처럼 처리) -> 이것을 검색할 때 사용\n",
    "- 원본을 요약하여 목차처럼 처리를 llm이 알아서 처리하도록하는게 정확할까? 아니면 사람이 하는게 정확할까?\n",
    "\t- Graph RAG가 이런 역할을 해준다.\n",
    "\t- 내용에서 LLM이 직접 관계성을 파악해서 추출 하는 방법이다.\n",
    "\t- chunking을 한 후 이 안에서 자세히 검색하는 방법\n",
    "\t\t- 확인 문서 : https://arxiv.org/pdf/2407.21059\n",
    "- 차주 진행할 것 \n",
    "\t- 답변 생성 시 관련성 없는 것을 미리 걸러낸 후 답변이 나오도록 처리\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 참조 문서 없이 직접 답변을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "template = '''Answer the question based only on the following context.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\n",
    "[Answer (in 한국어)]\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# 문서 포맷팅\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "# 검색기 생성 - 유사도 기반 상위 3개 문서 검색\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "\n",
    "# Chain 구성\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Chain 실행\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 참조 문서를 답변과 함께 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 문서와 포맷팅된 컨텍스트를 함께 반환하는 함수`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def get_context_and_docs(question: str) -> Dict:\n",
    "    \"\"\"문서와 포맷팅된 컨텍스트를 함께 반환\n",
    "    \n",
    "    Args:\n",
    "        question: 검색할 질문\n",
    "\n",
    "    Returns:\n",
    "        Dict: 문서와 포맷팅된 컨텍스트, 검색된 문서 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    # 검색 결과 가져오기\n",
    "    docs = retriever.invoke(question)\n",
    "    return {\n",
    "        \"question\": question,  # 질문\n",
    "        \"context\": format_docs(docs),   # 문서 포맷팅된 컨텍스트\n",
    "        \"source_documents\": docs   # 검색된 문서 리스트\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 컨텍스트와 질문을 입력으로 받아 답변을 생성하는 함수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def prompt_and_generate_answer(input_data: Dict) -> Dict:\n",
    "    \"\"\"컨텍스트와 질문을 입력으로 받아 답변을 생성\n",
    "\n",
    "    Args:\n",
    "        input_data (Dict): 컨텍스트와 질문이 포함된 딕셔너리\n",
    "\n",
    "    Returns:\n",
    "        Dict: 생성된 답변과 소스 문서 정보가 포함된 딕셔너리\n",
    "    \"\"\"\n",
    "\n",
    "    # LCEL 체인 구성 (StrOutputParser 사용)\n",
    "    answer_chain = prompt | llm | StrOutputParser()\n",
    "    answer = answer_chain.invoke({\n",
    "        \"question\":input_data[\"question\"],\n",
    "        \"context\":input_data[\"context\"]\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,  # 생성된 답변 (answer_chain 결과)\n",
    "        \"source_documents\": input_data[\"source_documents\"]  # 소스 문서 정보 (input_data에서 가져옴)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) RAG 체인 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "# Chain 구성\n",
    "rag_chain = (\n",
    "    RunnableLambda(get_context_and_docs) |  # 문서와 컨텍스트 가져오기 \n",
    "    {\n",
    "        'response': RunnableLambda(prompt_and_generate_answer), # 답변 생성\n",
    "\t\t###  \"source_documents\": input_data[\"source_documents\"] 코드로 question과 source_documents는 rag_chain에 전달 되므로 아래 코드는 불필요하다.\n",
    "        # 'question': itemgetter(\"question\"),  # 질문 그대로 전달\n",
    "        # \"source_documents\": itemgetter(\"source_documents\")   # 소스 반환\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 실행\n",
    "query = \"수원시의 주택건설지역은 어디에 해당하나요?\"\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"답변:\", result[\"response\"][\"answer\"])\n",
    "print(\"\\n참조 문서:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\n문서 {i}:\")\n",
    "    print(f\"내용: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 검색 문서 관련성 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 검색 문서와 질문 간의 관련성을 평가`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 문서의 질문 관련성 평가\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"주어진 컨텍스트가 질문에 답변하는데 필요한 정보를 포함하고 있는지 논리적으로 평가하세요.\n",
    "단계적으로 진행하며, 평가결과에 대한 검증을 수행하세요.\n",
    "\n",
    "다음 기준 중 하나 이상을 충족할 경우 'Yes'로 답변하고, 모두 충족하지 못하면 'No'로 답변하세요:\n",
    "\n",
    "1. 컨텍스트가 질문에 답변하는데 필요한 정보를 직접적으로 포함하고 있는가?\n",
    "2. 컨텍스트의 정보로부터 답변에 필요한 내용을 논리적으로 추론할 수 있는가?\n",
    "3. 컨텍스트의 정보가 질문에 대한 답변을 제공할 수 있는가?\n",
    "\n",
    "'Yes' 또는 'No'로만 답변하세요.\"\"\"),\n",
    "    (\"human\", \"\"\"[컨텍스트]\n",
    "{context}\n",
    "\n",
    "[질문]\n",
    "{question}\"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()    # gpt-4.1-mini 모델 사용\n",
    "\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\n문서 {i}:\")\n",
    "    print(f\"내용: {doc.page_content}\")\n",
    "    relevance = chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": query\n",
    "    }).lower()\n",
    "\n",
    "    print(f\"평가 결과: {relevance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4.1 모델 사용\n",
    "\n",
    "llm_gpt4o = ChatOpenAI(\n",
    "    model='gpt-4.1',          # 사용할 모델\n",
    "    temperature=0.1,          # 낮은 값: 일관된 답변 (0.0~2.0)\n",
    "    top_p=0.9,                # 토큰 샘플링 확률 임계값 (0.0~1.0)\n",
    ")\n",
    "\n",
    "chain = prompt | llm_gpt4o | StrOutputParser()    # gpt-4.1 모델 사용\n",
    "\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\n문서 {i}:\")\n",
    "    print(f\"내용: {doc.page_content}\")\n",
    "    relevance = chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": query\n",
    "    }).lower()\n",
    "\n",
    "    print(f\"평가 결과: {relevance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[실습] 문서 관련성 평가 체인을 구조화 출력으로 구현합니다.*** \n",
    "\n",
    "- pydantic schema 사용\n",
    "- with_structured_output 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio 챗봇 인터페이스 (RAG 시스템 클래스 구현)\n",
    "\n",
    "다음 클래스는 RAG 시스템의 전체 파이프라인을 캡슐화합니다.\n",
    "\n",
    "**주요 메서드**:\n",
    "- `_format_docs()`: 검색된 문서를 컨텍스트 문자열로 변환\n",
    "- `_format_source_documents()`: 참조 문서를 사용자 친화적 형식으로 포맷\n",
    "- `_evaluate_relevance()`: 검색된 문서의 질문 관련성 평가\n",
    "- `_generate_answer()`: 컨텍스트 기반 답변 생성\n",
    "- `generate_answer()`: Gradio 인터페이스용 메인 함수\n",
    "\n",
    "**클래스 구조**:\n",
    "1. LLM 초기화 (답변 생성용, 관련성 평가용)\n",
    "2. 벡터 스토어 검색기 설정\n",
    "3. 프롬프트 템플릿 정의\n",
    "4. RAG 체인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Optional, Generator\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    context: str\n",
    "    source_documents: Optional[List]\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(\n",
    "            self, \n",
    "            llm: BaseChatModel, \n",
    "            eval_llm: BaseChatModel,\n",
    "            retriever: VectorStoreRetriever\n",
    "        ):\n",
    "        if not llm:\n",
    "            self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "        else:\n",
    "            self.llm = llm\n",
    "\n",
    "        if not eval_llm:\n",
    "            self.eval_llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "        else:\n",
    "            self.eval_llm = eval_llm\n",
    "\n",
    "        if not retriever:\n",
    "            raise ValueError(\"검색기(retriever)가 필요합니다.\")\n",
    "        else:\n",
    "            self.retriever = retriever\n",
    "        \n",
    "    def _format_docs(self, docs: List) -> str:\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    def _format_source_documents(self, docs: Optional[List]) -> str:\n",
    "        if not docs:\n",
    "            return \"\\n\\nℹ️ 관련 문서를 찾을 수 없습니다.\"\n",
    "        \n",
    "        formatted_docs = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            metadata = doc.metadata if hasattr(doc, 'metadata') else {}\n",
    "            source_info = []\n",
    "            \n",
    "            if 'question_id' in metadata:\n",
    "                source_info.append(f\"ID: {metadata['question_id']}\")\n",
    "            if 'keyword' in metadata:\n",
    "                source_info.append(f\"키워드: {metadata['keyword']}\")\n",
    "            if 'summary' in metadata:\n",
    "                source_info.append(f\"요약: {metadata['summary']}\")\n",
    "                \n",
    "            formatted_docs.append(\n",
    "                f\"📚 참조 문서 {i}\\n\"\n",
    "                f\"• {' | '.join(source_info) if source_info else '출처 정보 없음'}\\n\"\n",
    "                f\"• 내용: {doc.page_content}\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs)\n",
    "    \n",
    "    def _check_relevance(self, docs: List, question: str) -> List:\n",
    "        \"\"\"문서의 관련성 확인\"\"\"\n",
    "\n",
    "        relevant_docs = []\n",
    "\n",
    "        if not docs:\n",
    "            return relevant_docs\n",
    "            \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"주어진 컨텍스트가 질문에 답변하는데 필요한 정보를 포함하고 있는지 평가하세요.\n",
    "\n",
    "        다음 기준 중 하나 이상을 충족할 경우 'Yes'로 답변하고, 모두 충족하지 못하면 'No'로 답변하세요:\n",
    "\n",
    "        1. 컨텍스트가 질문에 답변하는데 필요한 정보를 직접적으로 포함하고 있는가?\n",
    "        2. 컨텍스트의 정보로부터 답변에 필요한 내용을 논리적으로 추론할 수 있는가?\n",
    "\n",
    "        'Yes' 또는 'No'로만 답변하세요.\"\"\"),\n",
    "            (\"human\", \"\"\"[컨텍스트]\n",
    "        {context}\n",
    "\n",
    "        [질문]\n",
    "        {question}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.eval_llm | StrOutputParser()\n",
    "\n",
    "        for doc in docs:\n",
    "            result = chain.invoke({\n",
    "                \"context\": doc.page_content,\n",
    "                \"question\": question\n",
    "            }).lower()\n",
    "\n",
    "            print(f\"문서 {doc.metadata['question_id']} 관련성 확인 결과: {result}\")\n",
    "            print(f\"문서 {doc.metadata['question_id']} 내용:\")\n",
    "            print(doc.page_content)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            if \"yes\" in result:\n",
    "                relevant_docs.append(doc)\n",
    "            \n",
    "        return relevant_docs\n",
    "    \n",
    "    def search_documents(self, question: str) -> SearchResult:\n",
    "        try:\n",
    "            docs = retriever.invoke(question)\n",
    "            print(f\"검색된 문서 개수: {len(docs)}\")\n",
    "            relevant_docs = self._check_relevance(docs, question) \n",
    "            print(f\"관련 문서 개수: {len(relevant_docs)}\")\n",
    "            \n",
    "            return SearchResult(\n",
    "                context=self._format_docs(relevant_docs) if relevant_docs else \"관련 문서를 찾을 수 없습니다.\",\n",
    "                source_documents=relevant_docs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"문서 검색 중 오류 발생: {e}\")\n",
    "            return SearchResult(\n",
    "                context=\"문서 검색 중 오류가 발생했습니다.\",\n",
    "                source_documents=None,\n",
    "            )\n",
    "    \n",
    "    def generate_answer(self, message: str, history: List) -> Generator[str, None, None]:\n",
    "            \"\"\"Gradio 스트리밍 출력을 위한 제너레이터 함수\"\"\"\n",
    "            \n",
    "            # 1. 문서 검색 \n",
    "            search_result = self.search_documents(message)\n",
    "            \n",
    "            if not search_result.source_documents:\n",
    "                yield \"죄송합니다. 관련 문서를 찾을 수 없어 답변하기 어렵습니다. 다른 질문을 해주시겠습니까?\"\n",
    "                return\n",
    "                        \n",
    "            # 2. 프롬프트 템플릿 설정\n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"다음 지침을 따라 질문에 답변해주세요:\n",
    "                1. 주어진 문서의 내용만을 기반으로 답변하세요.\n",
    "                2. 문서에 명확한 근거가 없는 내용은 \"근거 없음\"이라고 답변하세요.\n",
    "                3. 답변하기 어려운 질문은 \"잘 모르겠습니다\"라고 답변하세요.\n",
    "                4. 추측이나 일반적인 지식을 사용하지 마세요.\"\"\"),\n",
    "                (\"human\", \"문서들:\\n{context}\\n\\n질문: {question}\")\n",
    "            ])\n",
    "            \n",
    "            # 3. RAG Chain 구성\n",
    "            chain = prompt | self.llm | StrOutputParser()\n",
    "            \n",
    "            full_answer = \"\"\n",
    "            try:\n",
    "                # 4. 스트리밍 실행 (chain.stream 사용)\n",
    "                for chunk in chain.stream({\n",
    "                    \"context\": search_result.context,\n",
    "                    \"question\": message\n",
    "                }):\n",
    "                    full_answer += chunk\n",
    "                    # 현재까지 생성된 텍스트를 Gradio UI에 즉시 반영\n",
    "                    yield full_answer\n",
    "                \n",
    "                # 5. 답변 생성이 완료된 후 참조 문서 추가\n",
    "                sources = self._format_source_documents(search_result.source_documents)\n",
    "                final_response = f\"{full_answer}\\n\\n---\\n{sources}\"\n",
    "                yield final_response\n",
    "                \n",
    "            except Exception as e:\n",
    "                yield f\"답변 생성 중 오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "\n",
    "rag_system = RAGSystem(\n",
    "    llm=ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0),  # 답변 생성에 사용할 모델\n",
    "    eval_llm=ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0), # 문서 관련성 평가에 사용할 모델\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    ")\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_system.generate_answer,\n",
    "    title=\"RAG QA 시스템\",\n",
    "    description=\"\"\"\n",
    "    질문을 입력하면 관련 문서를 검색하여 답변을 생성합니다.\n",
    "    모든 답변에는 참조한 문서의 출처가 표시됩니다.\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        [\"수원시의 주택건설지역은 어디에 해당하나요?\"],\n",
    "        [\"무주택 세대에 대해서 설명해주세요.\"],\n",
    "        [\"2순위로 당첨된 사람이 청약통장을 다시 사용할 수 있나요?\"],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 데모 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 종료\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **[실습] 주택청약 FAQ 시스템 구현**\n",
    "\n",
    "### **문제 설명**\n",
    "이전 코드를 기반으로 주택청약 FAQ 시스템을 다음 요구사항에 맞춰 개선합니다. \n",
    "\n",
    "1. 응답 품질 향상 (1개 이상)\n",
    "   - 생성된 답변의 품질을 평가 (답변이 불충분한 경우 예외 처리)\n",
    "   - 관련성 높은 FAQ 문서 검색 (임베딩 모델, 청크 크기, 벡터 검색 방법 등)\n",
    "\n",
    "2. 사용자 경험 개선 (1개 이상)\n",
    "   - 대화 이력 관리 기능 추가 (요약, 트리밍 기능 등 고려)\n",
    "   - 최근 대화 기반 컨텍스트 구성 \n",
    "   - 사용자 프로필 기반 맞춤 응답\n",
    "\n",
    "### **제약 조건**\n",
    "- Gradio ChatInterface 사용\n",
    "- RAG 구조 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "001-chatbot (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
