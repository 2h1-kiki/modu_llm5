{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2eb7b5f",
   "metadata": {},
   "source": [
    "# [실습 프로젝트] Naive RAG 구현 \n",
    "\n",
    "- 각 단계별 지시사항에 따라 코드를 완성하세요. \n",
    "- 제시된 지시사항과 LangChain 문서를 참조하여 시스템을 구성합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb79550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3단계: 문서 관리\n",
    "\n",
    "# 새로운 문서 추가\n",
    "new_doc = Document(\n",
    "    page_content=\"쿠버네티스 클러스터 운영 가이드\",\n",
    "    metadata={\"type\": \"tutorial\", \"author\": \"김미영\"}\n",
    ")\n",
    "new_id = str(uuid.uuid4())\n",
    "practice_db.add_documents(documents=[new_doc], ids=[new_id])\n",
    "print(f\"새 문서 추가 완료: {new_id}\")\n",
    "\n",
    "# 특정 문서 삭제 (첫 번째 문서 삭제)\n",
    "delete_id = doc_ids[0]\n",
    "practice_db.delete(ids=[delete_id])\n",
    "print(f\"문서 삭제 완료: {delete_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af65b2f",
   "metadata": {},
   "source": [
    "`(1) 벡터 저장소 설정`\n",
    "- HuggingFace에서 지원하는 BAAI/bge-m3 임베딩 모델을 사용하여 문서를 벡터화\n",
    "- FAISS DB를 벡터 스토어로 사용 (IndexFlatL2 사용: 유클리드 거리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings  \n",
    "\n",
    "# Hugging Face의 임베딩 모델 생성\n",
    "# 힌트: HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\") 사용\n",
    "embeddings_model = None\n",
    "\n",
    "# 임베딩 차원 확인\n",
    "embedding = embeddings_model.embed_query(\"test\")\n",
    "print(f\"임베딩 차원: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama 임베딩 모델을 사용한 FAISS 벡터 저장소 생성\n",
    "import faiss \n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# FAISS 인덱스 초기화 (유클리드 거리 사용)\n",
    "dim = 1024  # 임베딩 차원\n",
    "faiss_index = faiss.IndexFlatL2(dim)  \n",
    "\n",
    "# FAISS 벡터 저장소 생성\n",
    "faiss_db = None\n",
    "\n",
    "# 저장된 문서의 갯수 확인\n",
    "print(faiss_db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# 문서 id 생성\n",
    "doc_ids = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "\n",
    "# 문서를 벡터 저장소에 저장\n",
    "# 힌트: faiss_db.add_documents(chunks, ids=doc_ids) 사용\n",
    "added_doc_ids = None\n",
    "\n",
    "# 벡터 저장소에 저장된 문서를 확인\n",
    "print(f\"{len(added_doc_ids)}개의 문서가 성공적으로 벡터 저장소에 추가되었습니다.\")\n",
    "print(added_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be466ac",
   "metadata": {},
   "source": [
    "`(2) 검색기 정의`\n",
    "- mmr 검색으로 상위 3개 문서 검색하는 Retriever 사용\n",
    "- 다양성을 높이는 설정을 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmr 검색기 생성\n",
    "# 힌트: faiss_db.as_retriever(search_type='mmr', search_kwargs={'k': 3, 'fetch_k': 10, 'lambda_mult': 0.3})\n",
    "# lambda_mult를 낮게 설정하여 다양성을 높임\n",
    "faiss_mmr_retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 테스트 \n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "# 힌트: faiss_mmr_retriever.invoke(query) 사용\n",
    "retrieved_docs = None\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"-{i}-\\n{doc.page_content[:100]}...{doc.page_content[-100:]}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdac8f9",
   "metadata": {},
   "source": [
    "`(3) RAG 프롬프트 구성`\n",
    "\n",
    "- 작성 기준: \n",
    "    - LangChain의 ChatPromptTemplate 클래스 사용\n",
    "    - 변수 처리는 {context}, {question} 형식 사용\n",
    "    - 답변은 한글로 출력되도록 프롬프트 작성\n",
    "    \n",
    "- 아래 템플릿 코드를 기반으로 다음 내용을 참고하여 작성합니다. \n",
    "\n",
    "    1. 프롬프트 구성요소:\n",
    "        - 작업 지침\n",
    "        - 컨텍스트 영역\n",
    "        - 질문 영역\n",
    "        - 답변 형식 가이드\n",
    "\n",
    "    2. 작업 지침:\n",
    "        - 컨텍스트 기반 답변 원칙\n",
    "        - 외부 지식 사용 제한\n",
    "        - 불확실성 처리 방법\n",
    "        - 답변 불가능한 경우의 처리 방법\n",
    "\n",
    "    3. 답변 형식:\n",
    "        - 핵심 답변 섹션\n",
    "        - 근거 제시 섹션\n",
    "        - 추가 설명 섹션 (필요시)\n",
    "\n",
    "    4. 제약사항 반영:\n",
    "        - 답변은 사실에 기반해야 함\n",
    "        - 추측이나 가정을 최소화해야 함\n",
    "        - 명확한 근거 제시가 필요함\n",
    "        - 구조화된 형태로 작성되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb87d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 템플릿 (예시)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bcf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 템플릿 (여기에 작성하세요)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = None\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 템플릿 출력\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7e3f8",
   "metadata": {},
   "source": [
    "`(4) RAG 체인 구성`\n",
    "- LangChain의 LCEL 문법을 사용\n",
    "- 검색 결과를 프롬프트의 'context'로 전달하고,\n",
    "- 사용자가 입력한 질문을 그래도 프롬프트의 'question'에 전달\n",
    "- LLM 설정:\n",
    "    - ChatOpenAI 사용 ('gpt-4o-mini' 모델)\n",
    "    - temperature: 답변의 일관성을 가져가는 설정값을 사용 \n",
    "    - 기타 필요한 설정 \n",
    "- 출력 파서: 문자열 부분만 출력되도록 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d736e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 설정\n",
    "# 힌트: ChatOpenAI(model='gpt-4o-mini', temperature=0) 사용\n",
    "llm = None\n",
    "\n",
    "# 문서 포맷팅\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"{doc.page_content}\" for doc in docs])\n",
    "\n",
    "# RAG 체인 생성\n",
    "# 힌트: {'context': faiss_mmr_retriever | format_docs, 'question': RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "rag_chain = None\n",
    "\n",
    "# 체인 실행\n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "output = None\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"답변:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe159c82",
   "metadata": {},
   "source": [
    "`(5) Gradio 스트리밍 구현`\n",
    "- ChatInterface 사용\n",
    "- `chain.stream()`으로 응답을 청크 단위로 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Iterator\n",
    "\n",
    "# 스트리밍 응답 생성 함수\n",
    "def get_streaming_response(message: str, history) -> Iterator[str]:\n",
    "    \n",
    "    # RAG Chain 실행 및 스트리밍 응답 생성\n",
    "    response = \"\"\n",
    "    for chunk in rag_chain.stream(message):\n",
    "        if isinstance(chunk, str):\n",
    "            response += chunk\n",
    "            yield response\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "# 힌트: gr.ChatInterface(fn=get_streaming_response, title=\"RAG 기반 질의응답 시스템\", description=\"...\", examples=[...])\n",
    "demo = None\n",
    "\n",
    "# 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo 실행 종료\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
